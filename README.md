# ğŸ“ Classroom Audio-Visual Emotion Detection System

## Project Overview

**Classroom Austic Event** is an AI-powered system that analyzes student classroom behavior through two complementary modules:

1. **Audio Event Classification** - Detects classroom sounds and events (teacher speech, student discussion, silence, etc.)
2. **Emotion Detection** - Analyzes student facial expressions to detect emotional states

This system helps teachers understand classroom dynamics, student engagement levels, and identify students who may need attention or support.

---

## âœ¨ Key Features

### ğŸ”Š Audio Analysis
- Detects classroom events using YAMNet model
- Classifies sounds into categories:
  - Teacher Speech
  - Student Discussion
  - Silence/Background noise
  - Other classroom activities
- Processes audio files and real-time streams
- Stores analysis results in MongoDB

### ğŸ˜Š Emotion Detection
- **6 Emotion Categories**:
  - ğŸ˜Š **Attentive** - Student is focused and calm
  - ğŸ‘€ **Engaged** - Student is actively interested
  - ğŸ¤” **Confused** - Student is uncertain or questioning
  - ğŸ˜• **Distracted** - Student is disengaged
  - ğŸ¥± **Drowsy** - Student is tired/fatigued
  - ğŸ˜¤ **Frustrated** - Student is struggling
  - ğŸ˜° **Anxious** - Student is nervous/stressed
  - ğŸ˜‘ **Yawning** - Student is extremely tired

- **Multiple Input Methods**:
  - ğŸ“· Image upload (single frame analysis)
  - ğŸ¬ Video upload (multi-frame analysis)
  - ğŸ“¹ Webcam recording (live capture)

- **Visual Dashboard**:
  - Emoji indicators for each emotion
  - Color-coded results (Green=Good, Red=Problem, Purple=Alert)
  - Interactive pie charts
  - Percentage breakdown

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WEB INTERFACE (Flask)                     â”‚
â”‚              (HTML, CSS, JavaScript, Chart.js)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Audio Analysis  â”‚            â”‚ Emotion Detection    â”‚
    â”‚  Module          â”‚            â”‚ Module               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  YAMNet Model    â”‚            â”‚  DeepFace Model      â”‚
    â”‚  TensorFlow      â”‚            â”‚  (RetinaFace +       â”‚
    â”‚  (16kHz audio)   â”‚            â”‚   VGGFace2)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“                                    â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Custom Trained  â”‚            â”‚  Emotion Mapper      â”‚
    â”‚  Classification  â”‚            â”‚  (7 emotions â†’ 6+)   â”‚
    â”‚  Model           â”‚            â”‚                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“                                    â†“
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚   MongoDB Atlas (Cloud DB)       â”‚
          â”‚   â€¢ User credentials             â”‚
          â”‚   â€¢ Analysis results             â”‚
          â”‚   â€¢ Class details                â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤– Models Used

### 1. YAMNet Audio Classification Model
- **Source**: TensorFlow Hub
- **Purpose**: Pre-trained model for audio event classification
- **Input**: 16 kHz mono audio
- **Output**: 521 sound event classes with confidence scores
- **Framework**: TensorFlow 2.12.0

**Audio Processing Pipeline**:
```
Audio File â†’ Resample to 16kHz â†’ Extract Features â†’ 
YAMNet Model â†’ Classification â†’ Custom Training Model â†’ 
Classroom Event Labels
```

### 2. Custom Trained Classification Model
- **Source**: `trained_model.keras` (Keras/TensorFlow)
- **Purpose**: Maps YAMNet embeddings to classroom-specific event categories
- **Features**: Uses YAMNet's internal representations
- **Architecture**: Dense neural network layers
- **Performance**: Optimized for classroom environment sounds

### 3. DeepFace Emotion Detection
- **Source**: DeepFace 0.0.79 library
- **Face Detection**: RetinaFace (fast, accurate)
- **Emotion Recognition**: VGGFace2 backend
- **Input**: Images or video frames
- **Output**: 7 raw emotions (happy, sad, angry, surprise, fear, neutral, disgust)

**Emotion Processing Pipeline**:
```
Image/Video â†’ Face Detection (RetinaFace) â†’ 
Face Alignment â†’ Emotion Extraction (VGGFace2) â†’
Emotion Mapping â†’ Classroom Context (6+ emotions)
```

### 4. Emotion Mapping System
- **7 DeepFace Emotions** â†’ **6+ Classroom Emotions**
- **Mapping Logic**:
  ```
  happy       â†’ Engaged (active interest)
  neutral     â†’ Attentive (calm focus)
  sad         â†’ Drowsy (fatigue signals)
  angry       â†’ Frustrated (struggling)
  surprise    â†’ Confusion (uncertain)
  fear        â†’ Anxious (stressed)
  disgust     â†’ Distracted (disengaged)
  ```

---

## ğŸ“‹ Tech Stack

### Backend
- **Framework**: Flask 2.0.1
- **Python**: 3.10
- **Audio Processing**: librosa 0.9.2
- **ML Frameworks**:
  - TensorFlow 2.12.0
  - TensorFlow Hub 0.12.0
  - Keras 2.12.0
  - DeepFace 0.0.79
  - scikit-learn (preprocessing)

### Frontend
- **Languages**: HTML5, CSS3, JavaScript (Vanilla)
- **Visualization**: Chart.js (pie charts)
- **Media Capture**: MediaRecorder API, getUserMedia API

### Database
- **MongoDB Atlas** (Cloud)
- **Collections**:
  - `users` - User credentials
  - `class_details` - Analysis results and metadata

### Infrastructure
- **Web Server**: Flask (development)
- **Dependencies Management**: pip, virtualenv

---

## ğŸš€ Installation & Setup

### Prerequisites
- Windows/Linux/macOS
- Python 3.10
- pip (Python package manager)
- Virtual environment (recommended)

### Step 1: Clone the Repository
```bash
git clone https://github.com/Pradhyumna23/ClassRoom-Austic-event.git
cd ClassRoom-Austic-event
```

### Step 2: Create Virtual Environment
```bash
# Windows
python -m venv myenv
myenv\Scripts\activate

# Linux/macOS
python3 -m venv myenv
source myenv/bin/activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Download Required Models

#### A. YAMNet Model (Audio Classification)
```bash
# The model is automatically downloaded from TensorFlow Hub
# Or manually download and place in:
# C:/Users/ASUS/Desktop/yamnet-tensorflow2-yamnet-v1
```

**Note**: Update the path in `app.py` line 20 if you're using a different location:
```python
yamnet_model = tf.saved_model.load("YOUR_MODEL_PATH")
```

#### B. Trained Model (Already Included)
- `trained_model.keras` - Pre-trained classifier
- `label_encoder.npy` - Label mappings

### Step 5: Configure MongoDB (Optional)
Update the MongoDB connection string in `app.py` (line 47):
```python
MONGO_URI = "your_mongodb_connection_string"
```

If you don't have MongoDB:
- Create free account at [MongoDB Atlas](https://www.mongodb.com/cloud/atlas)
- Create a cluster
- Get connection string
- Or run app without database (reduced functionality)

### Step 6: Run the Application
```bash
python app.py
```

**Expected Output**:
```
Loading YAMNet model...
YAMNet model loaded successfully!
Loading trained model...
Trained model loaded successfully!
Loading label encoder...
Label encoder loaded successfully!
Connecting to MongoDB...
MongoDB connection successful!
 * Running on http://127.0.0.1:5000
```

### Step 7: Access the Web Interface
Open your browser and go to:
```
http://localhost:5000
```

---

## ğŸ“Š How It Works

### Audio Analysis Workflow
1. **Upload Audio File** (WAV, MP3, etc.)
2. **Preprocessing**:
   - Resample to 16 kHz (YAMNet requirement)
   - Normalize audio levels
   - Split into 10-second chunks
3. **Feature Extraction**:
   - YAMNet processes audio chunks
   - Extracts acoustic embeddings
4. **Classification**:
   - Custom model predicts classroom event
   - Generates confidence scores
5. **Storage**:
   - Results saved to MongoDB
   - Visualization in dashboard

### Emotion Detection Workflow
1. **Input Source**:
   - ğŸ“· Upload image, ğŸ¬ Upload video, or ğŸ“¹ Record from webcam
2. **Face Detection**:
   - RetinaFace detects all faces in frame
   - Extracts face regions
3. **Emotion Analysis**:
   - VGGFace2 extracts emotion features
   - Generates 7 emotion probabilities
4. **Emotion Mapping**:
   - Maps raw emotions to classroom context
   - Calculates averages (for videos)
   - Determines dominant emotion
5. **Visualization**:
   - Shows emoji indicators
   - Color-coded display
   - Pie chart with percentages
   - Detailed breakdown

### Webcam Recording Features
- **Start/Stop buttons** for user control
- **Live preview** during recording
- **Video preview** after recording stops
- **Automatic analysis** when recording stops
- **Real-time emotion display** with results

---

## ğŸ¯ Usage Examples

### Example 1: Audio Event Detection
```bash
1. Open http://localhost:5000
2. Go to "Audio Analysis" section
3. Click "Upload Audio"
4. Select an MP3 or WAV file
5. Click "Submit Audio Analysis"
6. View pie chart showing detected events:
   - Teacher Speech: 45%
   - Student Discussion: 30%
   - Background Noise: 15%
   - Silence: 10%
```

### Example 2: Emotion Detection (Image)
```bash
1. Open http://localhost:5000
2. Go to "Emotion Detection" section
3. Click "Upload Image"
4. Select a photo of a student
5. System analyzes:
   - Detects face(s)
   - Analyzes emotions
   - Maps to classroom emotions
6. View results:
   - ğŸ˜Š ATTENTIVE (75.50%)
   - ğŸ‘€ Engaged (15.25%)
   - ğŸ¤” Confused (9.25%)
```

### Example 3: Emotion Detection (Video)
```bash
1. Click "Upload Video"
2. Select a video file
3. System samples 10 frames from video
4. Analyzes each frame
5. Averages emotions across frames
6. Displays multi-frame analysis with timeline
```

### Example 4: Webcam Recording
```bash
1. Click "ğŸ”´ Start Recording" button
2. Live webcam feed appears
3. Speak or show expressions
4. Click "â¹ï¸ Stop Recording" button
5. Recording stops, video preview appears
6. Automatic emotion analysis begins
7. Results display with 6 emotions
```

---

## ğŸ“‚ Project Structure

```
ClassRoom-Austic-event/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ emotion_detector.py             # Emotion detection module
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ trained_model.keras             # Pre-trained audio classifier
â”œâ”€â”€ label_encoder.npy               # Audio label mappings
â”œâ”€â”€ style.css                       # Dashboard styling
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ index.html                  # Main dashboard
â”‚   â”œâ”€â”€ login.html                  # Login page
â”‚   â”œâ”€â”€ signup.html                 # Signup page
â”‚   â””â”€â”€ history.html                # Analysis history
â”œâ”€â”€ data/
â”‚   â””â”€â”€ audio_clip/                 # Sample audio files
â”‚       â”œâ”€â”€ STUDENT/                # Student sounds
â”‚       â”œâ”€â”€ TEACHER/                # Teacher sounds
â”‚       â”œâ”€â”€ DISTURBANCE/            # Background noise
â”‚       â””â”€â”€ BACKGROUND NOISE/       # Environmental sounds
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ Documentation/
    â”œâ”€â”€ EMOTION_ENHANCEMENT_PLAN.md
    â”œâ”€â”€ IMPLEMENTATION_STATUS.md
    â”œâ”€â”€ VISUAL_IMPLEMENTATION_GUIDE.md
    â””â”€â”€ ... (other guides)
```

---

## ğŸ”§ Configuration

### Audio Processing Settings (app.py)
```python
# Sample rate for audio processing
SAMPLE_RATE = 16000  # YAMNet requirement

# Audio chunk size
CHUNK_DURATION = 10  # seconds

# Number of video frames to analyze
SAMPLE_FRAMES = 10
```

### Emotion Detection Settings (emotion_detector.py)
```python
# Confidence threshold for emotion detection
MIN_CONFIDENCE = 0.0

# Video sampling
VIDEO_SAMPLE_FRAMES = 10

# Webcam recording duration (can be user-controlled)
WEBCAM_DURATION = 10  # seconds
```

### MongoDB Settings (app.py)
```python
# Database name
DATABASE_NAME = "class"

# Collections
USERS_COLLECTION = "users"
CLASS_COLLECTION = "class_details"
```

---

## ğŸ“ API Endpoints

### Audio Analysis
- **POST** `/upload` - Upload audio file
- **POST** `/submit` - Analyze uploaded audio
- **GET** `/history` - View analysis history

### Emotion Detection
- **POST** `/emotion-detect` - Analyze image emotions
- **POST** `/emotion-detect-video` - Analyze video emotions
- **POST** `/emotion-detect-webcam` - Analyze webcam recording

### User Management
- **POST** `/signup` - Create new user account
- **POST** `/login` - User login
- **GET** `/logout` - User logout

### Dashboard
- **GET** `/` - Main dashboard
- **GET** `/history` - View past analyses

---

## ğŸ“Š Expected Outputs

### Audio Analysis Output
```json
{
  "status": "success",
  "detected_events": {
    "Teacher Speech": 45.2,
    "Student Discussion": 30.1,
    "Background Noise": 15.3,
    "Silence": 9.4
  },
  "duration": 120.5,
  "confidence": 0.89
}
```

### Emotion Detection Output
```json
{
  "status": "success",
  "dominant_emotion": "attentive",
  "average_emotions": {
    "attentive": 75.50,
    "engaged": 15.25,
    "confused": 5.00,
    "distracted": 3.00,
    "drowsy": 1.00,
    "frustrated": 0.25
  },
  "emotion_breakdown": {
    "ğŸ˜Š Attentive": 75.50,
    "ğŸ‘€ Engaged": 15.25,
    "ğŸ¤” Confused": 5.00,
    "ğŸ˜• Distracted": 3.00,
    "ğŸ¥± Drowsy": 1.00,
    "ğŸ˜¤ Frustrated": 0.25
  }
}
```

---

## âš™ï¸ System Requirements

### Minimum Requirements
- **CPU**: Intel i5 or equivalent (for real-time processing)
- **RAM**: 8 GB (16 GB recommended for multiple concurrent analyses)
- **Disk Space**: 2 GB (for models and temporary files)
- **Network**: Internet connection (for MongoDB Atlas)

### Recommended Requirements
- **CPU**: Intel i7 or better
- **RAM**: 16 GB
- **GPU**: NVIDIA GPU with CUDA support (for faster processing)
- **Disk**: SSD with 5+ GB free space

---

## ğŸ› Troubleshooting

### Issue: YAMNet model not found
**Solution**:
```bash
# Download model from TensorFlow Hub or update path in app.py
# Or reinstall tensorflow-hub
pip install --upgrade tensorflow-hub
```

### Issue: DeepFace model download fails
**Solution**:
```bash
# Clear cache and reinstall
pip uninstall deepface -y
pip install deepface==0.0.79
```

### Issue: MongoDB connection fails
**Solution**:
1. Check MongoDB Atlas connection string
2. Verify IP whitelist in MongoDB settings
3. Or run without database (limited functionality)

### Issue: Audio file not processed
**Solution**:
1. Ensure audio is 16 kHz mono (resample if needed)
2. Check file format is supported (WAV, MP3, OGG)
3. Verify file is not corrupted

### Issue: Webcam not working
**Solution**:
1. Check browser permissions for camera access
2. Verify camera is not in use by another app
3. Try in different browser
4. Restart browser and try again

---

## ğŸ“ Logging & Debugging

### Enable Debug Mode
```python
# In app.py, set debug=True
if __name__ == '__main__':
    app.run(debug=True)
```

### Check Application Logs
Logs are printed to console and include:
- Model loading status
- Database connections
- Processing errors
- Analysis results

### Enable Verbose Output
```bash
# Run with verbose logging
FLASK_ENV=development FLASK_DEBUG=1 python app.py
```

---

## ğŸ“œ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ‘¥ Contributors

**Pradhyumna23** - Project Lead & Developer

## ğŸ“§ Contact

For questions, issues, or suggestions:
- GitHub: [Pradhyumna23/ClassRoom-Austic-event](https://github.com/Pradhyumna23/ClassRoom-Austic-event)
- Email: Contact through GitHub profile

---

## ğŸ‰ Features & Improvements

### Current Features
- âœ… Audio event classification (YAMNet + Custom model)
- âœ… Emotion detection with 6+ categories
- âœ… Webcam recording with live preview
- âœ… Image and video emotion analysis
- âœ… Color-coded visual display
- âœ… Emoji indicators
- âœ… MongoDB data storage
- âœ… Responsive web dashboard

### Future Enhancements
- ğŸ”„ Real-time streaming analysis
- ğŸ”„ Advanced analytics dashboard
- ğŸ”„ Class performance metrics
- ğŸ”„ Student engagement tracking
- ğŸ”„ Teacher behavior analysis
- ğŸ”„ Mobile application
- ğŸ”„ Multi-camera support
- ğŸ”„ Export reports functionality

---

## ğŸ“š Documentation

Comprehensive documentation available in the project:
- `EMOTION_ENHANCEMENT_PLAN.md` - Emotion detection details
- `IMPLEMENTATION_STATUS.md` - Implementation guide
- `VISUAL_IMPLEMENTATION_GUIDE.md` - Visual diagrams
- `QUICK_START.md` - Quick start guide
- And 7+ more detailed guides

---

## âœ… Verification Checklist

Before deploying to production:
- [ ] All models downloaded and paths configured
- [ ] MongoDB connection tested
- [ ] All dependencies installed correctly
- [ ] Application runs without errors
- [ ] Audio analysis working
- [ ] Emotion detection working
- [ ] Webcam recording tested
- [ ] Dashboard displays correctly

---

## ğŸ¯ Quick Start (TL;DR)

```bash
# 1. Clone & setup
git clone <repo>
cd ClassRoom-Austic-event
python -m venv myenv
myenv\Scripts\activate  # Windows or source myenv/bin/activate on Linux

# 2. Install & run
pip install -r requirements.txt
python app.py

# 3. Open browser
http://localhost:5000

# 4. Start analyzing!
# Upload audio/images/videos or use webcam
```

---

**Last Updated**: November 11, 2025  
**Version**: 2.0 (with Enhanced Emotion Detection)  
**Status**: Production Ready âœ…

ğŸ“ Happy Analyzing! ğŸ“
